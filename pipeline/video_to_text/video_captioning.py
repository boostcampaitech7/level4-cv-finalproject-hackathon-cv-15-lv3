import os
import json
import torch
from tqdm import tqdm
from contextlib import contextmanager, redirect_stdout, redirect_stderr
from PIL import Image
from transformers import AutoTokenizer, AutoProcessor, AutoConfig, AutoModel
from decord import VideoReader, cpu
from moviepy import VideoFileClip
from utils.translator import DeepLTranslator, DeepGoogleTranslator
from utils.tarsier_utils import load_model_and_processor
from utils.video_split import create_segmenter

@contextmanager
def suppress_output():
    """ëª¨ë“  ì¶œë ¥ì„ ì–µì œí•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    with open(os.devnull, 'w') as devnull:
        with redirect_stdout(devnull), redirect_stderr(devnull):
            yield

class MPLUGVideoCaptioningPipeline:
    def __init__(self, model_path='mPLUG/mPLUG-Owl3-7B-240728', keep_clips=False, 
                 segmentation_method="fixed", segmentation_params=None, mode='video2text',
                 video_metadata=None):
        # ê¸°ì¡´ ì´ˆê¸°í™” ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
        self.config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        self.model = AutoModel.from_pretrained(
            model_path,
            attn_implementation='sdpa',
            torch_dtype=torch.half,
            trust_remote_code=True
        )
        self.model.eval().cuda()
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.processor = self.model.init_processor(self.tokenizer)
        self.MAX_NUM_FRAMES = 16
        
        self.keep_clips = keep_clips
        self.mode = mode
        
        # ì„¸ê·¸ë©˜í„° ì´ˆê¸°í™” ì¶”ê°€
        segmentation_params = segmentation_params or {"segment_duration": 5}
        self.segmenter = create_segmenter(
            method=segmentation_method, 
            **segmentation_params
        )
        
        # ë‚˜ë¨¸ì§€ ì´ˆê¸°í™” ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
        self.output_dir = f"output/{mode}"
        self.clips_dir = f"clips/{mode}"
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.clips_dir, exist_ok=True)
        
        self.video_mapping = {}
        self.video_counter = 1
        self.clip_counter = 1
        self.video_name_to_id = {}
        self.translator = DeepGoogleTranslator()

        self.video_metadata = video_metadata

    def generate_segments(self, video_path):
        """Generate segments for a video using the selected segmentation method"""
        return self.segmenter.get_segments(video_path)


    def _extract_clip(self, video_path, start_time, end_time):
        """Extract clip from video using timestamp"""
        clip = VideoFileClip(video_path).subclipped(start_time, end_time)
        clip_name = f"{os.path.basename(video_path).rsplit('.', 1)[0]}_clip{self.clip_counter}.mp4"
        clip_path = os.path.join(self.clips_dir, clip_name)
        clip.write_videofile(clip_path, codec='libx264', audio=False)
        clip.close()
        return clip_path

    def _encode_video(self, video_path):
        """Encode video frames for processing"""
        def uniform_sample(l, n):
            gap = len(l) / n
            idxs = [int(i * gap + gap / 2) for i in range(n)]
            return [l[i] for i in idxs]

        vr = VideoReader(video_path, ctx=cpu(0))
        sample_fps = round(vr.get_avg_fps() / 1)
        frame_idx = [i for i in range(0, len(vr), sample_fps)]
        if len(frame_idx) > self.MAX_NUM_FRAMES:
            frame_idx = uniform_sample(frame_idx, self.MAX_NUM_FRAMES)
        frames = vr.get_batch(frame_idx).asnumpy()
        frames = [Image.fromarray(v.astype('uint8')) for v in frames]
        return frames

    def _generate_caption(self, video_frames):
        """Generate caption for video frames"""
        messages = [
            {"role": "user", "content": "<|video|> Describe this video in detail."},
            {"role": "assistant", "content": ""}
        ]

        inputs = self.processor(messages, images=None, videos=[video_frames])
        inputs.to('cuda')
        inputs.update({
            'tokenizer': self.tokenizer,
            'max_new_tokens': 200,
            'decode_text': True,
        })

        return self.model.generate(**inputs)[0]

    def process_videos(self, video_list):
        """Process list of videos and generate captions"""
        results = []

        for video_path, start_time, end_time in video_list:
            if video_path not in self.video_name_to_id:
                self.video_name_to_id[video_path] = f"video{self.video_counter}"
                self.video_mapping[self.video_name_to_id[video_path]] = {
                    "title": os.path.basename(video_path),
                    "full_path": video_path,
                    "clips": []
                }
                self.video_counter += 1

            video_id = self.video_name_to_id[video_path]
            clip_id = f"clip{self.clip_counter}"

            # Extract clip
            clip_path = self._extract_clip(video_path, start_time, end_time)

            try:
                # Encode and generate caption
                video_frames = self._encode_video(clip_path)
                caption = self._generate_caption(video_frames)
                
                # video2text ëª¨ë“œì¼ ë•Œë§Œ í•œê¸€ ë²ˆì—­ ì¶”ê°€
                if self.mode == "video2text":
                    caption_ko = self.translator.translate_en_to_ko(caption)
                else:
                    caption_ko = None

                # Add clip info to mapping with clip path
                self.video_mapping[video_id]["clips"].append({
                    "clip_id": clip_id,
                    "start_time": start_time,
                    "end_time": end_time,
                    "clip_path": clip_path
                })

                # Create result entry
                entry = {
                    "video_path": video_path,
                    "video_id": video_id,
                    "clip_id": clip_id,
                    "start_time": start_time,
                    "end_time": end_time,
                    "caption": caption
                }
                
                # video2text ëª¨ë“œì¼ ë•Œë§Œ í•œê¸€ ìº¡ì…˜ ì¶”ê°€
                if caption_ko:
                    entry["caption_ko"] = caption_ko
                
                self.clip_counter += 1
                results.append(entry)

            finally:
                # Only remove clip if keep_clips is False
                if not self.keep_clips and os.path.exists(clip_path):
                    os.remove(clip_path)

        return results
    
    def get_video_duration(self, video_path):
        """Get video duration using moviepy"""
        clip = VideoFileClip(video_path)
        duration = clip.duration
        clip.close()
        return duration

    def generate_segments(self, video_path):
        """Generate segments for a video with fixed duration"""
        duration = self.get_video_duration(video_path)
        segments = []
        start_time = 0
        
        while start_time < duration:
            end_time = min(start_time + self.segment_duration, duration)
            if end_time - start_time >= 1:  # ìµœì†Œ 1ì´ˆ ì´ìƒì¸ ì„¸ê·¸ë¨¼íŠ¸ë§Œ í¬í•¨
                segments.append((start_time, end_time))
            start_time = end_time
        
        return segments

    def process_directory(self, videos_dir):
        """Process all MP4 files in the directory"""
        video_list = []
        for file in os.listdir(videos_dir):
            if file.lower().endswith('.mp4'):
                video_path = os.path.join(videos_dir, file)
                print(f"Processing video: {file}")
                
                # Generate segments for this video
                segments = self.generate_segments(video_path)
                
                # Add segments to video list
                video_list.extend([
                    (video_path, start, end)
                    for start, end in segments
                ])
        
        if not video_list:
            print("Error: No valid videos found to process")
            return None
            
        print(f"Total segments to process: {len(video_list)}")
        return self.process_videos(video_list)

    def save_results(self, results):
        """Save results to JSON files in mode-specific output directory"""
        # Save results with mode-specific names
        if self.mode == "video2text":
            captions_file = "v2t_captions.json"
            mapping_file = "v2t_mapping.json"
        else:  # text2video
            captions_file = "t2v_captions.json"
            mapping_file = "t2v_mapping.json"

        # Save captions
        output_path = os.path.join(self.output_dir, captions_file)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, indent=4, ensure_ascii=False, fp=f)

        # Save video mapping
        mapping_path = os.path.join(self.output_dir, mapping_file)
        with open(mapping_path, 'w', encoding='utf-8') as f:
            json.dump(self.video_mapping, indent=4, ensure_ascii=False, fp=f)


def find_video_file(videos_dir, video_name):
    """Find video file in directory by name (ignoring extension)"""
    for file in os.listdir(videos_dir):
        if file.rsplit('.', 1)[0] == video_name:
            return os.path.join(videos_dir, file)
    return None


class TarsierVideoCaptioningPipeline:
    def __init__(self, model_path, keep_clips=False, segmentation_method="fixed", 
                 segmentation_params=None, mode='video2text', video_metadata=None):
        # Model initialization
        self.model, self.processor = load_model_and_processor(model_path, max_n_frames=8)
        self.model.eval()
        
        self.keep_clips = keep_clips
        self.mode = mode
        
        # ì„¸ê·¸ë©˜í„° ì´ˆê¸°í™”
        segmentation_params = segmentation_params or {}
        self.segmenter = create_segmenter(
            method=segmentation_method, 
            **segmentation_params
        )
        
        # Create output and clips directories
        self.output_dir = f"output/{mode}"
        self.clips_dir = f"clips/{mode}"
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.clips_dir, exist_ok=True)
        
        # Initialize counters and mappings
        self.clip_counter = 0
        self.video_mapping = {}
        
        self.translator = DeepGoogleTranslator()

        self.video_metadata = video_metadata

    def generate_segments(self, video_path):
        """Generate segments for a video using the selected segmentation method"""
        return self.segmenter.get_segments(video_path)

    def generate_caption(self, video_path):
        """Generate caption for a video clip using Tarsier"""
        instructions = [
            "<video>\nDescribe the video in detail."
        ]
        
        captions = []
        for instruction in instructions:
            try:
                caption = self._generate_single_caption(video_path, instruction)
                captions.append(caption)
            except Exception as e:
                print(f"ğŸš¨ ìº¡ì…˜ ìƒì„± ì˜¤ë¥˜: {str(e)}")
                continue
        
        # Combine captions
        final_caption = " ".join(captions)
        return final_caption

    def _generate_single_caption(self, video_path, instruction):
        """Generate a single caption with given instruction"""
        inputs = self.processor(instruction, video_path, edit_prompt=True, return_prompt=True)
        if 'prompt' in inputs:
            inputs.pop('prompt')
        
        inputs = {k: v.to(self.model.device) for k, v in inputs.items() if v is not None}
        
        outputs = self.model.generate(
            **inputs,
            do_sample=True,
            max_new_tokens=512,
            top_p=0.9,
            temperature=0.8,
            use_cache=True
        )
        
        output_text = self.processor.tokenizer.decode(
            outputs[0][inputs['input_ids'][0].shape[0]:], 
            skip_special_tokens=True
        )
        return output_text

    def process_video(self, video_path, start_time, end_time):
        """Process a video segment and generate caption"""
        video_name = os.path.basename(video_path)  # video_XXX.mp4
        video_id = video_name.split('_')[1]  # XXX ë¶€ë¶„ ì¶”ì¶œ
        clip_id = f"clip_{self.clip_counter + 1}"
        
        # ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        metadata = self.video_metadata.get(video_name, {})
        
        # Extract clip
        clip_path = os.path.join(self.clips_dir, f"{video_id}_{clip_id}.mp4")
        try:
            with VideoFileClip(video_path) as video:
                clip = video.subclipped(start_time, end_time)
                clip.write_videofile(clip_path, codec='libx264', audio=False)
        except Exception as e:
            print(f"ğŸš¨ í´ë¦½ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return None
        
        # Generate caption
        caption = self.generate_caption(clip_path)
        if not caption:
            return None
            
        # Translate caption to Korean if in video2text mode
        caption_ko = None
        if self.mode == "video2text":
            caption_ko = self.translator.translate_en_to_ko(caption)
        
        # Create result entry with metadata
        result = {
            "video_path": f"video_{video_id}/{self.clip_counter:05d}.mp4",  # video_XXX/00001.mp4 í˜•ì‹
            "video_id": metadata.get('video_id', video_id),  # ë©”íƒ€ë°ì´í„°ì—ì„œ ì—†ìœ¼ë©´ íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œí•œ ID ì‚¬ìš©
            "title": metadata.get('title', ''),
            "url": metadata.get('url', ''),
            "start_time": f"{start_time:.2f}",
            "end_time": f"{end_time:.2f}",
            "caption": caption
        }
        
        if caption_ko:
            result["caption_ko"] = caption_ko
        
        # Update video mapping
        if video_id not in self.video_mapping:
            self.video_mapping[video_id] = {
                "video_path": video_path,
                "clips": []
            }
        
        self.video_mapping[video_id]["clips"].append({
            "clip_id": f"{self.clip_counter:05d}",
            "start_time": start_time,
            "end_time": end_time,
            "clip_path": clip_path
        })
        
        self.clip_counter += 1
        
        # Clean up if needed
        if not self.keep_clips and os.path.exists(clip_path):
            os.remove(clip_path)
            
        return result

    def process_videos(self, video_list):
        """Process list of videos"""
        results = []
        for video_path, start_time, end_time in video_list:
            result = self.process_video(video_path, start_time, end_time)
            if result:
                results.append(result)
        return results

    def process_directory(self, videos_dir):
        """Process all videos in directory"""
        video_list = []
        print("ğŸ“‚ ë¹„ë””ì˜¤ ëª©ë¡ ìƒì„± ì¤‘...")
        
        # ë¹„ë””ì˜¤ íŒŒì¼ ëª©ë¡ ìƒì„±
        video_files = [f for f in os.listdir(videos_dir) if f.endswith(('.mp4', '.avi', '.mov'))]
        for file in tqdm(video_files, desc="ì„¸ê·¸ë¨¼íŠ¸ ë¶„í• "):
            video_path = os.path.join(videos_dir, file)
            segments = self.segmenter.get_segments(video_path)
            for start_time, end_time in segments:
                video_list.append((video_path, start_time, end_time))
        
        print(f"ì´ {len(video_files)}ê°œ ë¹„ë””ì˜¤, {len(video_list)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ë°œê²¬")
        
        # ë¹„ë””ì˜¤ ì²˜ë¦¬
        results = []
        pbar = tqdm(total=len(video_list), desc="ë¹„ë””ì˜¤ ì²˜ë¦¬")
        for video_path, start_time, end_time in video_list:
            with suppress_output():
                result = self.process_video(video_path, start_time, end_time)
                if result:
                    results.append(result)
            pbar.update(1)
        pbar.close()
        
        return results

    def save_results(self, results):
        """Save results to JSON files"""
        if self.mode == "video2text":
            captions_file = "v2t_captions.json"
            mapping_file = "v2t_mapping.json"
        else:
            captions_file = "t2v_captions.json"
            mapping_file = "t2v_mapping.json"

        output_path = os.path.join(self.output_dir, captions_file)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=4, ensure_ascii=False)

        mapping_path = os.path.join(self.output_dir, mapping_file)
        with open(mapping_path, 'w', encoding='utf-8') as f:
            json.dump(self.video_mapping, f, indent=4, ensure_ascii=False)
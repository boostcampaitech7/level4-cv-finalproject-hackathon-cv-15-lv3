import argparse
import json
import os
import sys
import time
from tqdm import tqdm
from moviepy import VideoFileClip
from utils.translator import DeepLTranslator, DeepGoogleTranslator
from video_to_text.video_captioning import MPLUGVideoCaptioningPipeline, TarsierVideoCaptioningPipeline
from text_to_video.embedding import FaissSearch

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def save_search_result_clip(video_path, start_time, end_time, output_dir, clip_name):
    """ê²€ìƒ‰ ê²°ê³¼ í´ë¦½ì„ ì €ì¥"""
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        clip = VideoFileClip(video_path).subclipped(start_time, end_time)
        output_path = os.path.join(output_dir, f"{clip_name}.mp4")
        clip.write_videofile(output_path, codec='libx264', audio=False)
        clip.close()
        
        print(f"âœ… ê²€ìƒ‰ ê²°ê³¼ í´ë¦½ ì €ì¥ ì™„ë£Œ: {output_path}")
        return output_path
        
    except Exception as e:
        print(f"âŒ í´ë¦½ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def text_to_video_search(query_text, model_type="mplug"):
    """í…ìŠ¤íŠ¸ë¡œ ë¹„ë””ì˜¤ ê²€ìƒ‰í•˜ëŠ” íŒŒì´í”„ë¼ì¸"""
    print("\nğŸš€ í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
    start_time = time.time()
    
    # ì„¤ì • ê°’ ë¡œë“œ
    print("âš™ï¸ ì„¤ì • ë¡œë“œ ì¤‘...")
    VIDEOS_DIR = "../videos"
    KEEP_CLIPS = False
    current_dir = os.path.dirname(os.path.abspath(__file__))
    model_path = os.path.abspath(os.path.join(current_dir, "..", "..", "Tarsier-7b"))

    # ë©”íƒ€ë°ì´í„° ë¡œë“œ
    print("ğŸ“‚ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì¤‘...")
    with open('../videos/sample.json', 'r', encoding='utf-8') as f:
        video_metadata = {item['video_name']: item for item in json.load(f)}

    # VideoCaptioningPipeline ì´ˆê¸°í™”
    print(f"ğŸ”§ {model_type.upper()} ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    model_init_time = time.time()
    
    if model_type == "mplug":
        pipeline = MPLUGVideoCaptioningPipeline(
            keep_clips=KEEP_CLIPS,
            segmentation_method="fixed",
            segmentation_params={"segment_duration": 5},
            mode="text2video",
            video_metadata=video_metadata
        )
    else:
        pipeline = TarsierVideoCaptioningPipeline(
            model_path=model_path,
            keep_clips=KEEP_CLIPS,
            segmentation_method="fixed",
            segmentation_params={"segment_duration": 5},
            mode="text2video",
            video_metadata=video_metadata
        )
    
    print(f"â±ï¸ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ ({time.time() - model_init_time:.1f}ì´ˆ)")
    
    # ë¹„ë””ì˜¤ ì²˜ë¦¬
    print("\nğŸ¬ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘...")
    process_time = time.time()
    results = pipeline.process_directory(VIDEOS_DIR)
    if results:
        pipeline.save_results(results)
    print(f"â±ï¸ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ ({time.time() - process_time:.1f}ì´ˆ)")
    
    # FAISS ê²€ìƒ‰
    print("\nğŸ” FAISS ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    search_time = time.time()
    translator = DeepGoogleTranslator()
    faiss_search = FaissSearch(json_path=f"output/text2video/t2v_captions.json")
    
    print(f"ğŸ” ê²€ìƒ‰ì–´: '{query_text}'")
    similar_captions = faiss_search.find_similar_captions(query_text, translator, top_k=1)
    print(f"â±ï¸ ê²€ìƒ‰ ì™„ë£Œ ({time.time() - search_time:.1f}ì´ˆ)")
    
    # ê²°ê³¼ ì¶œë ¥ ë° í´ë¦½ ì €ì¥
    search_results_dir = "output/text2video/search_results"
    
    for i, (caption, similarity, video_info) in enumerate(similar_captions):
        print(f"\nğŸ¯ ê²€ìƒ‰ ê²°ê³¼ {i+1}")
        print(f"ğŸ“Š ìœ ì‚¬ë„: {similarity:.4f}")
        print(f"ğŸ¬ ë¹„ë””ì˜¤: {os.path.basename(video_info['video_path'])}")
        print(f"â° êµ¬ê°„: {video_info['start_time']}ì´ˆ ~ {video_info['end_time']}ì´ˆ")
        print(f"ğŸ“ ì œëª©: {video_info['title']}")
        print(f"ğŸ“ ìº¡ì…˜: {caption}")
        
        clip_name = f"search_result_{i+1}_{os.path.basename(video_info['video_path']).split('.')[0]}"
        saved_path = save_search_result_clip(
            video_info['video_path'],
            video_info['start_time'],
            video_info['end_time'],
            search_results_dir,
            clip_name
        )
    
    total_time = time.time() - start_time
    print(f"\nâœ¨ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ (ì´ {total_time:.1f}ì´ˆ)")

def video_to_text_process(model_type="mplug"):
    """ë¹„ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” íŒŒì´í”„ë¼ì¸"""
    print("\nğŸš€ ë¹„ë””ì˜¤-í…ìŠ¤íŠ¸ ë³€í™˜ íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
    start_time = time.time()
    
    # ì„¤ì • ê°’
    print("âš™ï¸ ì„¤ì • ë¡œë“œ ì¤‘...")
    VIDEOS_DIR = "../videos"
    INPUT_JSON = "video_to_text/input_table.json"
    KEEP_CLIPS = False
    current_dir = os.path.dirname(os.path.abspath(__file__))
    model_path = os.path.abspath(os.path.join(current_dir, "..", "..", "Tarsier-7b"))

    # ë©”íƒ€ë°ì´í„° ë¡œë“œ
    print("ğŸ“‚ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì¤‘...")
    load_time = time.time()
    with open('../videos/sample.json', 'r', encoding='utf-8') as f:
        video_metadata = {item['video_name']: item for item in json.load(f)}
    print(f"â±ï¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({time.time() - load_time:.1f}ì´ˆ)")

    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    print(f"ğŸ”§ {model_type.upper()} ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    model_init_time = time.time()
    
    if model_type == "mplug":
        pipeline = MPLUGVideoCaptioningPipeline(
            keep_clips=KEEP_CLIPS,
            mode="video2text",
            video_metadata=video_metadata
        )
    else:  # tarsier
        pipeline = TarsierVideoCaptioningPipeline(
            model_path=model_path,
            keep_clips=KEEP_CLIPS,
            mode="video2text",
            video_metadata=video_metadata
        )
    print(f"â±ï¸ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ ({time.time() - model_init_time:.1f}ì´ˆ)")
    
    # JSON íŒŒì¼ì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ ë¡œë“œ
    print("\nğŸ“‚ ì…ë ¥ JSON ë¡œë“œ ì¤‘...")
    json_load_time = time.time()
    with open(INPUT_JSON, 'r') as f:
        input_data = json.load(f)
    print(f"â±ï¸ JSON ë¡œë“œ ì™„ë£Œ ({time.time() - json_load_time:.1f}ì´ˆ)")
    
    # ë¹„ë””ì˜¤ ì²˜ë¦¬
    print("\nğŸ¬ ë¹„ë””ì˜¤ ëª©ë¡ ìƒì„± ì¤‘...")
    video_list = []
    for video_data in tqdm(input_data['videos'], desc="ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤€ë¹„"):
        video_path = os.path.join(VIDEOS_DIR, f"{video_data['video_name']}.mp4")
        if not os.path.exists(video_path):
            print(f"âš ï¸ ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {video_data['video_name']}")
            continue
        
        video_list.extend([
            (video_path, seg['start'], seg['end'])
            for seg in video_data['segments']
        ])
    
    if not video_list:
        print("âŒ ì²˜ë¦¬í•  ë¹„ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë¹„ë””ì˜¤ ì²˜ë¦¬ ë° ìº¡ì…˜ ìƒì„±
    print(f"\nğŸ¥ ì´ {len(video_list)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì¤‘...")
    process_time = time.time()
    results = pipeline.process_videos(video_list)
    print(f"â±ï¸ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ ({time.time() - process_time:.1f}ì´ˆ)")
    
    # ê²°ê³¼ ì €ì¥
    print("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
    save_time = time.time()
    pipeline.save_results(results)
    print(f"â±ï¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ ({time.time() - save_time:.1f}ì´ˆ)")
    
    total_time = time.time() - start_time
    print(f"\nâœ¨ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ (ì´ {total_time:.1f}ì´ˆ)")
    print(f"ğŸ“Š ì²˜ë¦¬ëœ ì„¸ê·¸ë¨¼íŠ¸: {len(results)}/{len(video_list)}")

def main():
    parser = argparse.ArgumentParser(description='Video Processing Pipeline')
    parser.add_argument('mode', choices=['text2video', 'video2text'],
                      help='Choose pipeline mode: text2video or video2text')
    parser.add_argument('--model', choices=['mplug', 'tarsier'], default='tarsier',
                      help='Choose model type: mplug or tarsier (default: tarsier)')
    
    args = parser.parse_args()
    
    if args.mode == 'text2video':
        # ê²€ìƒ‰í•  í…ìŠ¤íŠ¸ ì§ì ‘ ì§€ì •
        query_text = "ë‚¨ì ì–¼êµ´ ìœ„ì— ê±°ë¯¸ê°€ ì˜¬ë¼ê°€ì„œ ë‚¨ìê°€ ë†€ë¼ëŠ” ì¥ë©´"
        text_to_video_search(query_text, model_type=args.model)
    else:
        video_to_text_process(model_type=args.model)

if __name__ == "__main__":
    main()
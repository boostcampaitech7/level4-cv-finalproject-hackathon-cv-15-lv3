import os
import json
from config import Config
from sentence_transformers import SentenceTransformer
from tarsier_utils import load_model_and_processor
import torch

def process():
    # ëª¨ë¸ í•œ ë²ˆë§Œ ë¡œë“œ
    print("ğŸ¤– Tarsier ëª¨ë¸ ë¡œë”© ì¤‘...")
    model_path = "/data/ephemeral/home/Tarsier-7b"
    model, processor = load_model_and_processor(model_path)
    
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    print("ğŸ”¤ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
    embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    
    video_files = os.listdir(Config.video_dir)
    results = []
    
    print(f"ì´ {len(video_files)}ê°œì˜ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘...")
    
    # ë°°ì¹˜ í¬ê¸° ì„¤ì •
    batch_size = 2
    
    for i in range(0, len(video_files), batch_size):
        video_paths = video_files[i:i + batch_size]
        
        # Ensure paths are valid
        batch_video_paths = [
            os.path.join(Config.video_dir, video_file)
            for video_file in video_paths
            if os.path.exists(os.path.join(Config.video_dir, video_file))
        ]
        
        try:
            # ìº¡ì…˜ ìƒì„± (ë¯¸ë¦¬ ë¡œë“œí•œ ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ì „ë‹¬)
            instruction = "<video>\nDescribe the video in detail."
            
            # ê° ë¹„ë””ì˜¤ì— ëŒ€í•´ ì…ë ¥ì„ ì¤€ë¹„
            inputs_list = []
            for video_path in batch_video_paths:
                inputs = processor(instruction, video_path, edit_prompt=True, return_prompt=True)
                inputs.pop('prompt', None)
                inputs = {k: v.to(model.device) for k, v in inputs.items() if v is not None}
                inputs_list.append(inputs)
    
            # ë°°ì¹˜ ì…ë ¥ ìƒì„± (ê° ì…ë ¥ì„ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬)
            batch_inputs = {k: torch.cat([inputs[k] for inputs in inputs_list], dim=0) for k in inputs_list[0]}
            
            outputs = model.generate(
                **batch_inputs,
                do_sample=True,
                max_new_tokens=512,
                top_p=0.9,
                temperature=0.8,
                use_cache=True
            )
            
            for idx, video_file in enumerate(video_paths):
                caption = processor.tokenizer.decode(
                    outputs[idx][inputs_list[idx]['input_ids'][0].shape[0]:], 
                    skip_special_tokens=True
                )
                
                if not caption:
                    continue
                    
                # ì„ë² ë”© ìƒì„±
                embedding = embedding_model.encode([caption])[0]
                
                # ê²°ê³¼ ì €ì¥
                result = {
                    "video_path": video_file,
                    "video_name": '_'.join(os.path.splitext(video_file)[0].split('_')[:-2]),
                    "start_time": float(os.path.splitext(video_file)[0].split('_')[-2]),
                    "end_time": float(os.path.splitext(video_file)[0].split('_')[-1]),
                    "caption": caption,
                    "embedding": embedding.tolist()
                }
                results.append(result)
                print(f"âœ“ {video_file} ì²˜ë¦¬ ì™„ë£Œ")
                
        except Exception as e:
            print(f"âœ— {video_file} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            continue

    # JSON íŒŒì¼ë¡œ ì €ì¥
    with open(Config.output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=4, ensure_ascii=False)

    print(f"\nì´ {len(results)}/{len(video_files)}ê°œì˜ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ")
    print(f"ê²°ê³¼ê°€ {Config.output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


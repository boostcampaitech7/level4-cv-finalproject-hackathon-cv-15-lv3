import faiss
import json
import os
import numpy as np
from sentence_transformers import SentenceTransformer
from deep_translator import GoogleTranslator

class DeepGoogleTranslator:
    """deep-translator ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ í•œêµ­ì–´ â†”ï¸ ì˜ì–´ ë²ˆì—­ê¸° í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.ko_to_en = GoogleTranslator(source='ko', target='en')
        self.en_to_ko = GoogleTranslator(source='en', target='ko')
    
    def translate_ko_to_en(self, text):
        """í•œêµ­ì–´ â†’ ì˜ì–´ ë²ˆì—­"""
        try:
            return self.ko_to_en.translate(text)
        except Exception as e:
            print(f"ğŸš¨ ë²ˆì—­ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def translate_en_to_ko(self, text):
        """ì˜ì–´ â†’ í•œêµ­ì–´ ë²ˆì—­"""
        try:
            return self.en_to_ko.translate(text)
        except Exception as e:
            print(f"ğŸš¨ ë²ˆì—­ ì˜¤ë¥˜: {str(e)}")
            return None

# all-mpnet-base-v2
# all-MiniLM-L6-v2
class FaissSearch:
    """FAISS ê¸°ë°˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ í´ë˜ìŠ¤"""
    
    def __init__(self, json_path, model_name="all-MiniLM-L6-v2", use_gpu=True):
        self.json_path = json_path
        self.translator = DeepGoogleTranslator()
        self.model = SentenceTransformer(model_name)  # SentenceTransformer ëª¨ë¸ ì„¤ì •

        # JSON ë°ì´í„° ë¡œë“œ
        if os.path.exists(self.json_path):
            with open(self.json_path, 'r', encoding='utf-8') as f:
                self.data = json.load(f)
        else:
            print(f"ğŸš¨ ì˜¤ë¥˜: {self.json_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return

        # `embedding`ì´ ì—†ëŠ” í•­ëª© ê±´ë„ˆë›°ê¸°
        self.filtered_data = [entry for entry in self.data if "embedding" in entry and entry["embedding"]]
        
        if not self.filtered_data:
            print("ğŸš¨ `embedding`ì´ í¬í•¨ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        # í•„í„°ë§ëœ ë°ì´í„°ë¡œ ì„ë² ë”© ë°°ì—´ ìƒì„±
        self.captions = [entry["caption"] for entry in self.filtered_data]
        self.embeddings = np.array([entry["embedding"] for entry in self.filtered_data], dtype=np.float32)
        faiss.normalize_L2(self.embeddings)

        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        self._initialize_faiss(use_gpu)
    
    def _initialize_faiss(self, use_gpu):
        """FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„± ë° ì´ˆê¸°í™”"""
        self.dimension = self.embeddings.shape[1]
        self.res = faiss.StandardGpuResources() if use_gpu else None
        self.index = faiss.IndexFlatIP(self.dimension)
        self.gpu_index = faiss.index_cpu_to_gpu(self.res, 0, self.index) if use_gpu else self.index
        self.gpu_index.add(self.embeddings)
    
    def find_similar_captions(self, input_text, top_k=3):
        """í•œêµ­ì–´ ì…ë ¥ â†’ ì˜ì–´ ë³€í™˜ â†’ FAISS ê²€ìƒ‰ â†’ í•œêµ­ì–´ ë³€í™˜ í›„ ê²°ê³¼ ë°˜í™˜"""
        translated_query = self.translator.translate_ko_to_en(input_text)
        # translated_query = input_text
        if not translated_query:
            print("ğŸš¨ ë²ˆì—­ ì‹¤íŒ¨! ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return []

        # SentenceTransformerë¡œ ì„ë² ë”© ìƒì„±
        query_embedding = self.model.encode([translated_query], convert_to_numpy=True, normalize_embeddings=True)
        query_embedding = np.array(query_embedding, dtype=np.float32)

        if query_embedding.shape[1] != self.dimension:
            raise ValueError(f"ğŸš¨ ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜! Query: {query_embedding.shape[1]}, FAISS Index: {self.dimension}")

        D, I = self.gpu_index.search(query_embedding, top_k)

        results = []
        for idx, i in enumerate(I[0]):
            caption_ko = self.translator.translate_en_to_ko(self.captions[i])
            video_info = {
                'video_path': self.filtered_data[i].get('video_path', 'N/A'),
                'video_id': self.filtered_data[i].get('video_id', 'N/A'),
                'title': self.filtered_data[i].get('title', 'Unknown Title'),
                'url': self.filtered_data[i].get('url', 'N/A'),
                'start_time': self.filtered_data[i].get('start_time', 0),
                'end_time': self.filtered_data[i].get('end_time', 0)
            }
            results.append((caption_ko, D[0][idx], video_info))

        return results
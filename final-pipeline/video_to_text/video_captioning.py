import os
import json
import torch
import time
from tqdm import tqdm
from contextlib import contextmanager, redirect_stdout, redirect_stderr
from PIL import Image
from transformers import AutoTokenizer, AutoProcessor, AutoConfig, AutoModel
from decord import VideoReader, cpu
from moviepy import VideoFileClip
from utils.translator import DeepLTranslator, DeepGoogleTranslator
from utils.tarsier_utils import load_model_and_processor
from utils.video_split import create_segmenter

@contextmanager
def suppress_output():
    """ëª¨ë“  ì¶œë ¥ì„ ì–µì œí•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    with open(os.devnull, 'w') as devnull:
        with redirect_stdout(devnull), redirect_stderr(devnull):
            yield

def find_video_file(videos_dir, video_name):
    """Find video file in directory by name (ignoring extension)"""
    for file in os.listdir(videos_dir):
        if file.rsplit('.', 1)[0] == video_name:
            return os.path.join(videos_dir, file)
    return None


class TarsierVideoCaptioningPipeline:
    def __init__(self, model_path, keep_clips=False, segmentation_method="fixed", 
                 segmentation_params=None, mode='video2text', video_metadata=None, clips_dir=None):
        # Model initialization
        self.model, self.processor = load_model_and_processor(model_path, max_n_frames=8)
        self.model.eval()
        
        self.keep_clips = keep_clips
        self.mode = mode
        
        # ì„¸ê·¸ë©˜í„° ì´ˆê¸°í™”
        segmentation_params = segmentation_params or {}
        self.segmenter = create_segmenter(
            method=segmentation_method, 
            **segmentation_params
        )
        
        # Create output and clips directories
        self.output_dir = f"output/{mode}"
        self.clips_dir = clips_dir or f"clips/{mode}"  # ì™¸ë¶€ì—ì„œ ì§€ì •í•œ clips_dir ì‚¬ìš©
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.clips_dir, exist_ok=True)
        
        # Initialize counters and mappings
        self.clip_counter = 0
        self.video_mapping = {}
        
        self.translator = DeepLTranslator()

        self.video_metadata = video_metadata

    def generate_segments(self, video_path):
        """Generate segments for a video using the selected segmentation method"""
        return self.segmenter.get_segments(video_path)

    def generate_caption(self, video_path):
        """Generate caption for a video clip using Tarsier"""
        instructions = [
            "<video>\nDescribe the video in detail."
        ]
        
        captions = []
        for instruction in instructions:
            try:
                caption = self._generate_single_caption(video_path, instruction)
                captions.append(caption)
            except Exception as e:
                print(f"ğŸš¨ ìº¡ì…˜ ìƒì„± ì˜¤ë¥˜: {str(e)}")
                continue
        
        # Combine captions
        final_caption = " ".join(captions)
        return final_caption

    def _generate_single_caption(self, video_path, instruction):
        """Generate a single caption with given instruction"""
        inputs = self.processor(instruction, video_path, edit_prompt=True, return_prompt=True)
        if 'prompt' in inputs:
            inputs.pop('prompt')
        
        inputs = {k: v.to(self.model.device) for k, v in inputs.items() if v is not None}
        
        outputs = self.model.generate(
            **inputs,
            do_sample=True,
            max_new_tokens=512,
            top_p=0.9,
            temperature=0.8,
            use_cache=True
        )
        
        output_text = self.processor.tokenizer.decode(
            outputs[0][inputs['input_ids'][0].shape[0]:], 
            skip_special_tokens=True
        )
        return output_text

    def process_video(self, video_path, start_time, end_time):
        """Process a video segment and generate caption"""
        video_name = os.path.basename(video_path)  # video_XXX.mp4
        if video_name.startswith('video_'):
            video_id = video_name.split('.')[0]  # video_XXX ë¶€ë¶„ ì¶”ì¶œ (í™•ì¥ì ì œê±°)
        else:
            video_id = os.path.splitext(video_name)[0]  # í™•ì¥ì ì œê±°

        clip_id = f"{video_id}_{int(start_time)}_{int(end_time)}"  # í´ë¦½ ID í˜•ì‹ ë³€ê²½
        
        # Extract clip
        clip_path = os.path.join(self.clips_dir, f"{clip_id}.mp4")  # ì—¬ê¸°ì„œ í´ë¦½ íŒŒì¼ëª… ê²°ì •
        try:
            with suppress_output(), VideoFileClip(video_path) as video:
                clip = video.subclipped(start_time, end_time)
                clip.write_videofile(clip_path, codec='libx264', audio=False)
        except Exception as e:
            print(f"ğŸš¨ í´ë¦½ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return None
        
        # ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        metadata = self.video_metadata.get(video_name, {})

        # Generate caption
        with suppress_output():  # ìº¡ì…˜ ìƒì„± ë¡œê·¸ ì–µì œ
            caption = self.generate_caption(clip_path)
        if not caption:
            return None
            
        # Translate caption to Korean if in video2text mode
        caption_ko = None
        if self.mode == "video2text":
            caption_ko = self.translator.translate_en_to_ko(caption)
        
        # Create result entry with metadata
        result = {
            "video_path": f"video_{video_id}/{self.clip_counter:05d}.mp4",
            "video_id": metadata.get('video_id', ''),
            "title": metadata.get('title', video_id),
            "url": metadata.get('url', ''),
            "start_time": f"{start_time:.2f}",
            "end_time": f"{end_time:.2f}",
            "caption": caption,
            "clip_path": clip_path  # ì‹¤ì œ ì €ì¥ëœ í´ë¦½ ê²½ë¡œ ì¶”ê°€
        }
        
        if caption_ko:
            result["caption_ko"] = caption_ko
        
        # Update video mapping
        if video_id not in self.video_mapping:
            self.video_mapping[video_id] = {
                "video_path": video_path,
                "clips": []
            }
        
        self.video_mapping[video_id]["clips"].append({
            "clip_id": clip_id,
            "start_time": start_time,
            "end_time": end_time,
            "clip_path": clip_path
        })
        
        self.clip_counter += 1
        
        # keep_clipsê°€ Falseì¸ ê²½ìš°ì—ë§Œ ì‚­ì œ
        if not self.keep_clips and os.path.exists(clip_path):
            os.remove(clip_path)
            
        return result

    def process_videos(self, video_list):
        """Process list of videos"""
        results = []
        for video_path, start_time, end_time in video_list:
            result = self.process_video(video_path, start_time, end_time)
            if result:
                results.append(result)
        return results

    def process_directory(self, videos_dir):
        """Process all videos in directory with detailed monitoring"""
        video_stats = {
            'total_videos': 0,
            'total_segments': 0,
            'total_duration': 0,
            'clip_extraction_time': 0,
            'caption_generation_time': 0,
            'total_success': 0,
            'total_failed': 0
        }
        
        # 1. ë¹„ë””ì˜¤ ë¶„ì„
        print("\nğŸ“Š ë¹„ë””ì˜¤ ë¶„ì„ ì¤‘...")
        analysis_start = time.time()
        video_files = [f for f in os.listdir(videos_dir) if f.endswith(('.mp4', '.avi', '.mov'))]
        
        for file in tqdm(video_files, desc="ë¹„ë””ì˜¤ ì •ë³´ ìˆ˜ì§‘"):
            try:
                with suppress_output():  # VideoFileClip ë¡œê·¸ ì–µì œ
                    with VideoFileClip(os.path.join(videos_dir, file)) as video:
                        video_stats['total_duration'] += video.duration
                        video_stats['total_videos'] += 1
            except Exception as e:
                print(f"âš ï¸ {file} ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        
        analysis_time = time.time() - analysis_start
        print(f"âœ“ ë¹„ë””ì˜¤ ë¶„ì„ ì™„ë£Œ ({analysis_time:.1f}ì´ˆ)")
        print(f"â€¢ ì´ ë¹„ë””ì˜¤: {video_stats['total_videos']}ê°œ")
        print(f"â€¢ ì´ ê¸¸ì´: {video_stats['total_duration']/60:.1f}ë¶„")

        # 2. ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
        print("\nğŸ”„ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  ì¤‘...")
        segment_start = time.time()
        video_list = []
        
        for file in tqdm(video_files, desc="ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±"):
            video_path = os.path.join(videos_dir, file)
            try:
                with suppress_output():  # ì„¸ê·¸ë¨¼í„° ë¡œê·¸ ì–µì œ
                    segments = self.segmenter.get_segments(video_path)
                    video_stats['total_segments'] += len(segments)
                    video_list.extend([(video_path, start, end) for start, end in segments])
            except Exception as e:
                print(f"âš ï¸ {file} ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        
        segment_time = time.time() - segment_start
        print(f"âœ“ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± ì™„ë£Œ ({segment_time:.1f}ì´ˆ)")
        print(f"â€¢ ì´ ì„¸ê·¸ë¨¼íŠ¸: {video_stats['total_segments']}ê°œ")
        print(f"â€¢ í‰ê·  ì„¸ê·¸ë¨¼íŠ¸/ë¹„ë””ì˜¤: {video_stats['total_segments']/video_stats['total_videos']:.1f}ê°œ")

        # 3. ë¹„ë””ì˜¤ ì²˜ë¦¬
        results = []
        print("\nğŸ¬ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘...")
        process_start = time.time()
        
        pbar = tqdm(total=len(video_list), desc="ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬")
        for video_path, start_time, end_time in video_list:
            clip_start = time.time()
            with suppress_output():  # ë¹„ë””ì˜¤ ì²˜ë¦¬ ë¡œê·¸ ì–µì œ
                result = self.process_video(video_path, start_time, end_time)
            
            if result:
                results.append(result)
                video_stats['total_success'] += 1
            else:
                video_stats['total_failed'] += 1
                
            video_stats['clip_extraction_time'] += time.time() - clip_start
            pbar.update(1)
        pbar.close()
        
        process_time = time.time() - process_start
        
        # ìµœì¢… í†µê³„
        print("\nğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
        print(f"â€¢ ì´ ì†Œìš” ì‹œê°„: {process_time:.1f}ì´ˆ")
        print(f"â€¢ í´ë¦½ ì²˜ë¦¬ ì‹œê°„: {video_stats['clip_extraction_time']:.1f}ì´ˆ")
        print(f"â€¢ ì„±ê³µ: {video_stats['total_success']}/{video_stats['total_segments']}ê°œ")
        print(f"â€¢ ì‹¤íŒ¨: {video_stats['total_failed']}ê°œ")
        print(f"â€¢ í‰ê·  ì²˜ë¦¬ ì†ë„: {video_stats['total_duration']/process_time:.1f}ì´ˆ/ì´ˆ")
        
        return results

    def save_results(self, results):
        """Save results to JSON files"""
        if self.mode == "video2text":
            captions_file = "v2t_captions.json"
            mapping_file = "v2t_mapping.json"
        else:
            captions_file = "t2v_captions.json"
            mapping_file = "t2v_mapping.json"

        output_path = os.path.join(self.output_dir, captions_file)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=4, ensure_ascii=False)

        mapping_path = os.path.join(self.output_dir, mapping_file)
        with open(mapping_path, 'w', encoding='utf-8') as f:
            json.dump(self.video_mapping, f, indent=4, ensure_ascii=False)
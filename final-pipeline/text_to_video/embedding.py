import faiss
import json
import os
import numpy as np
import requests
import time
from tqdm import tqdm
from sentence_transformers import SentenceTransformer


class FaissSearch:
    """FAISS ê¸°ë°˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ í´ë˜ìŠ¤"""
    # all-MiniLM-L6-v2, all-mpnet-base-v2
    def __init__(self, json_path, model_name="all-MiniLM-L6-v2", use_gpu=True):
        init_start = time.time()
        print("\nğŸ”§ FAISS ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        self.json_path = json_path
        
        # 1. ëª¨ë¸ ë¡œë“œ
        model_start = time.time()
        print("ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.model = SentenceTransformer(model_name)
        self.model.to("cuda")
        self.model.eval()
        print(f"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({time.time() - model_start:.1f}ì´ˆ)")

        # 2. JSON ë°ì´í„° ë¡œë“œ
        json_start = time.time()
        print("ğŸ“‚ JSON ë°ì´í„° ë¡œë“œ ì¤‘...")
        if os.path.exists(self.json_path):
            with open(self.json_path, 'r', encoding='utf-8') as f:
                self.data = json.load(f)
            print(f"âœ“ JSON ë¡œë“œ ì™„ë£Œ ({time.time() - json_start:.1f}ì´ˆ)")
            
            # 3. ì„ë² ë”© ìƒì„± ë˜ëŠ” ë¡œë“œ
            embedding_start = time.time()
            if "embedding" not in self.data[0]:
                print("ğŸ”„ ì„ë² ë”© ë²¡í„° ìƒì„± ì¤‘...")
                for entry in tqdm(self.data, desc="ì„ë² ë”© ìƒì„±"):
                    entry["embedding"] = self.model.encode(entry["caption"]).tolist()
                
                # ì„ë² ë”© ì €ì¥
                with open(self.json_path, 'w', encoding='utf-8') as f:
                    json.dump(self.data, f, indent=4, ensure_ascii=False)
            print(f"âœ“ ì„ë² ë”© ì²˜ë¦¬ ì™„ë£Œ ({time.time() - embedding_start:.1f}ì´ˆ)")
        else:
            print(f"ğŸš¨ ì˜¤ë¥˜: {self.json_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return

        # 4. FAISS ì¸ë±ìŠ¤ ìƒì„±
        faiss_start = time.time()
        print("ğŸ” FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        self.captions = [entry["caption"] for entry in self.data]
        self.embeddings = np.array([entry["embedding"] for entry in self.data], dtype=np.float32)
        faiss.normalize_L2(self.embeddings)
        self._initialize_faiss(use_gpu)
        print(f"âœ“ FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ ({time.time() - faiss_start:.1f}ì´ˆ)")
        
        print(f"\nâœ¨ ì´ˆê¸°í™” ì™„ë£Œ (ì´ {time.time() - init_start:.1f}ì´ˆ)")
        print(f"â€¢ ì´ ìº¡ì…˜ ìˆ˜: {len(self.data)}ê°œ")

    def _load_json_data(self):
        """JSON íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ìº¡ì…˜ ë° ì„ë² ë”©ì„ ê°€ì ¸ì˜´"""
        with open(self.json_path, "r", encoding="utf-8") as f:
            self.data = json.load(f)
        self.captions = [entry["caption"] for entry in self.data]
        self.embeddings = np.array([entry["embedding"] for entry in self.data], dtype=np.float32)
        faiss.normalize_L2(self.embeddings)

    def _initialize_faiss(self, use_gpu):
        """FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„± ë° ì´ˆê¸°í™”"""
        self.dimension = self.embeddings.shape[1]
        self.res = faiss.StandardGpuResources() if use_gpu else None
        self.index = faiss.IndexFlatIP(self.dimension)
        self.gpu_index = faiss.index_cpu_to_gpu(self.res, 0, self.index) if use_gpu else self.index
        self.gpu_index.add(self.embeddings)

    def find_similar_captions(self, input_text, translator, top_k=3):
        """ê²€ìƒ‰ ì‹œê°„ ëª¨ë‹ˆí„°ë§ì´ ì¶”ê°€ëœ ê²€ìƒ‰ í•¨ìˆ˜"""
        search_start = time.time()
        
        # 1. ë²ˆì—­
        translate_start = time.time()
        translated_query = translator.translate_ko_to_en(input_text)
        translate_time = time.time() - translate_start
        
        print(f"ğŸ” ê²€ìƒ‰ì–´: '{input_text}'")
        print(f"ğŸ” ë²ˆì—­ëœ ê²€ìƒ‰ì–´: '{translated_query}'")
        
        if not translated_query:
            print("ğŸš¨ ê²€ìƒ‰ì–´ ë²ˆì—­ ì‹¤íŒ¨!")
            return []
        
        # 2. ì„ë² ë”© ìƒì„±
        embed_start = time.time()
        query_embedding = self.model.encode([translated_query]).astype(np.float32)
        faiss.normalize_L2(query_embedding)
        embed_time = time.time() - embed_start
        
        # 3. FAISS ê²€ìƒ‰
        search_start_time = time.time()
        D, I = self.gpu_index.search(query_embedding, top_k)
        search_time = time.time() - search_start_time
        
        # 4. ê²°ê³¼ ì²˜ë¦¬
        results = []
        process_start = time.time()
        for idx, i in enumerate(I[0]):
            # try:
            #     caption_ko = ''
            #     if not caption_ko:  # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì˜ì–´ ìº¡ì…˜ ì‚¬ìš©
            #         print(f"âš ï¸ ìº¡ì…˜ ë²ˆì—­ ì‹¤íŒ¨ - ì˜ì–´ ìº¡ì…˜ ì‚¬ìš©: {self.captions[i][:100]}...")
            #         caption_ko = self.captions[i]
            # except Exception as e:
            #     print(f"âš ï¸ ìº¡ì…˜ ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ - ì˜ì–´ ìº¡ì…˜ ì‚¬ìš©: {str(e)}")
            #     caption_ko = self.captions[i]
                
            video_folder = self.data[i]['video_path'].split('/')[0]
            video_name = f"{video_folder}.mp4"
            real_video_path = os.path.join("../videos", video_name)
            
            video_info = {
                'video_path': real_video_path,
                'video_id': self.data[i]['video_id'],
                'title': self.data[i]['title'],
                'url': self.data[i]['url'],
                'start_time': float(self.data[i]['start_time']),
                'end_time': float(self.data[i]['end_time'])
            }
            results.append((D[0][idx], video_info))
        
        process_time = time.time() - process_start
        total_time = time.time() - search_start
        
        print("\nâ±ï¸ ê²€ìƒ‰ ì„±ëŠ¥:")
        print(f"â€¢ ë²ˆì—­ ì‹œê°„: {translate_time:.3f}ì´ˆ")
        print(f"â€¢ ì„ë² ë”© ì‹œê°„: {embed_time:.3f}ì´ˆ")
        print(f"â€¢ ê²€ìƒ‰ ì‹œê°„: {search_time:.3f}ì´ˆ")
        print(f"â€¢ ê²°ê³¼ ì²˜ë¦¬ ì‹œê°„: {process_time:.3f}ì´ˆ")
        print(f"â€¢ ì´ ì†Œìš” ì‹œê°„: {total_time:.3f}ì´ˆ")
        
        return results

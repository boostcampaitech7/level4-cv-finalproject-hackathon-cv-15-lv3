import torch
import numpy as np
import pandas as pd
import faiss
import json
import os
import time
import av
import multiprocessing.pool  # âœ… ì •ì‹ ThreadPool ì‚¬ìš©
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import clip
from tqdm import tqdm

def set_cuda(gpu):
    torch.cuda.set_device(gpu)
    device = torch.device(f'cuda:{gpu}')
    print(f"CUDA available: {torch.cuda.is_available()}")
    print(f"Current device: {torch.cuda.current_device()}\n")
    return device

# GPU ì„¤ì • (RTX 3090 ìµœì í™”)
device = set_cuda(0)

# CLIP ëª¨ë¸ ë¡œë“œ (ë©€í‹° GPU ì§€ì›)
clip_model, clip_preprocess = clip.load("ViT-B/32", device=device)
clip_model = torch.nn.DataParallel(clip_model)

# BLIP-2 ëª¨ë¸ ë¡œë“œ (FP16 ìµœì í™”)
blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device).half()

# âœ… GPU ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜
def clear_cuda_cache():
    torch.cuda.empty_cache()

# 1. PyAV(FFmpeg ê¸°ë°˜) ì‚¬ìš©í•˜ì—¬ í”„ë ˆì„ ì €ì¥ ì—†ì´ ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘ ë¡œë“œ (FPS 4 ì ìš©, ìµœëŒ€ 300 í”„ë ˆì„ ì œí•œ)
def extract_frames_memory_pyav(video_path, fps=4):
    start_time_exec = time.time()

    container = av.open(video_path)
    video_stream = container.streams.video[0]
    video_stream.thread_type = "AUTO"

    frames = []
    timestamps = []
    frame_interval = max(1, int(video_stream.time_base * fps))  

    print(f"ğŸ¥ [í”„ë ˆì„ ì¶”ì¶œ] {video_path} - FPS: {fps}")
    for frame in tqdm(container.decode(video=0), desc="Extracting frames"):
        if frame.pts % frame_interval == 0:
            img = frame.to_ndarray(format="rgb24")  # RGB ë³€í™˜
            timestamp = float(frame.pts * video_stream.time_base)  # ì´ˆ ë‹¨ìœ„ ì‹œê°„ ë³€í™˜
            frames.append(img)
            timestamps.append((timestamp, timestamp + (1 / fps)))

    container.close()

    # âœ… ìµœëŒ€ 300ê°œ í”„ë ˆì„ê¹Œì§€ë§Œ ì‚¬ìš© (ë©”ëª¨ë¦¬ ìµœì í™”)
    frames = frames[:300]
    timestamps = timestamps[:300]

    end_time_exec = time.time()
    print(f"âœ… [í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ] {video_path} - {len(frames)}ê°œ í”„ë ˆì„, ì‹¤í–‰ ì‹œê°„: {end_time_exec - start_time_exec:.2f}s")
    return frames, timestamps

# 2. BLIP-2 ë°°ì¹˜ ì²˜ë¦¬ ì ìš© (batch_size=1ë¡œ ë‚®ì¶¤)
def generate_captions(frames, relevant_timestamps, batch_size=1):
    start_time_exec = time.time()
    
    captions = {}
    num_batches = len(frames) // batch_size + 1

    print("ğŸ“ [ìº¡ì…˜ ìƒì„± ì¤‘]...")
    for i in tqdm(range(num_batches), desc="Generating captions"):
        batch_frames = frames[i * batch_size:(i + 1) * batch_size]
        batch_images = [Image.fromarray(frame) for frame in batch_frames]

        if len(batch_images) == 0:
            continue

        inputs = blip_processor(images=batch_images, return_tensors="pt", padding=True).to(device)
        caption_ids = blip_model.generate(**inputs, max_length=50, num_beams=3)
        caption_texts = blip_processor.batch_decode(caption_ids, skip_special_tokens=True)

        for j, caption in enumerate(caption_texts):
            captions[relevant_timestamps[i * batch_size + j]] = caption

    end_time_exec = time.time()
    print(f"âœ… [ìº¡ì…”ë‹ ì™„ë£Œ] ì‹¤í–‰ ì‹œê°„: {end_time_exec - start_time_exec:.2f}s")

    return captions

# âœ… 3. ê°œë³„ ì˜ìƒ ë¶„ì„ í•¨ìˆ˜
def process_single_video(video_path, query_list, relevance_threshold=0.2, fps=4):
    total_start_time = time.time()
    video_name = os.path.basename(video_path).split(".")[0]
    print(f"â–¶ Processing video: {video_name}")

    frames, timestamps = extract_frames_memory_pyav(video_path, fps=fps)
    
    # âœ… GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    clear_cuda_cache()

    results_json = {"video": video_name, "results": []}

    for query in query_list:
        print(f"ğŸ”¹ Query: {query}")
        query_embedding = clip_model.module.encode_text(clip.tokenize([query]).to(device)).detach().cpu().numpy()

        frame_embeddings = []
        print("ğŸ¯ [CLIP ì„ë² ë”© ì¶”ì¶œ ì¤‘]...")
        for frame in tqdm(frames, desc="Extracting CLIP embeddings"):
            image = clip_preprocess(Image.fromarray(frame)).unsqueeze(0).to(device)
            image_embedding = clip_model.module.encode_image(image).detach().cpu().numpy()
            frame_embeddings.append(image_embedding)

        frame_embeddings = np.vstack(frame_embeddings)

        # FAISS ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰
        index = faiss.IndexFlatL2(frame_embeddings.shape[1])
        index.add(frame_embeddings)
        distances, indices = index.search(query_embedding, len(frames))

        relevant_timestamps = []
        for i, dist in zip(indices[0], distances[0]):
            if dist < relevance_threshold:
                relevant_timestamps.append(timestamps[i])

        captions = generate_captions(frames, relevant_timestamps)

        for (start_time, end_time), caption in captions.items():
            result_entry = {
                "query": query,
                "start_time": start_time,
                "end_time": end_time,
                "caption_en": caption
            }
            results_json["results"].append(result_entry)

    total_end_time = time.time()
    print(f"âœ… [ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ] {video_name} - ì´ ì‹¤í–‰ ì‹œê°„: {total_end_time - total_start_time:.2f}s")

# âœ… 4. ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰ (ë©€í‹°í”„ë¡œì„¸ì‹± OFF)
if __name__ == "__main__":
    video_list = [
        "/home/hwang/leem/level4-cv-finalproject-hackathon-cv-15-lv3/json/videos/new_video_9.mp4",
    ]
    query_list = ["A woman in pink clothes wearing glasses is teaching art to students in school uniforms in the classroom."]

    # âœ… ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰í•˜ì—¬ ë©”ëª¨ë¦¬ ì´ìŠˆ í™•ì¸
    for video in tqdm(video_list, desc="Processing videos"):
        process_single_video(video, query_list, 0.2, 4)